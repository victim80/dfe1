<div class="eg-content-editor" data-type="imageEditorOneColumn"><div class="column">
  <div class="cropped-image" style="position:relative;overflow: hidden;padding-top: 56.022727272727266%">
    <img style="position: absolute; top: 50%;left: 50%;transform: translate(-50%, -50%);max-width: 100%; max-height: 100%;" src="https://files-storage.easygenerator.com/image/3b0d1bc7-2958-488b-9ec4-35e1ddb4720c.png" alt="" data-src="https://files-storage.easygenerator.com/image/5d595228-23c4-4fd7-99ac-1cc64807202f.jpg" data-init="{&quot;w&quot;:880,&quot;h&quot;:493,&quot;y&quot;:0,&quot;x&quot;:0,&quot;defaultScale&quot;:1.375,&quot;scale&quot;:0.6875}">
  </div>
  <div class="row" data-content-type="TextEditor"><p style="text-align:justify;height:normal;">Due to the distributed nature of web applications, traces of activities are recorded across numerous hardware and software components. Web applications serve a wide range of services and can support various types of servers such as IIS and Apache. Therefore, forensic investigators must have good knowledge of various servers to examine the logs and understand them when an incident occurs.</p><p style="text-align:justify;height:normal;">Web applications are often business critical. Therefore, it is difficult for investigators to create their forensic image, which requires the website to be down for some time. This makes it a challenge for investigators to capture volatile data including processes, port/network connections, logs of memory dumps, and user logs at the time of the incident analysis. On the other hand, as the traffic of websites increases, the log files recorded in the database keep increasing. So, it becomes difficult for the investigators to collect and analyze these logs.</p><p style="text-align:justify;height:normal;">Investigators must have a good understanding of all types of web and applications servers to understand, analyze, and correlate various formats of logs collected from their respective sources. When a website attack occurs, investigators must gather the digital fingerprints left by the attacker. However, tracing back becomes a difficult job as attackers often use reverse proxies and anonymizers. Investigators also need to collect the following data fields associated with each HTTP request made to the website to understand how the attack was performed:</p><ul><li style="text-align: justify;">Date and time at which the request was sent</li><li style="text-align: justify;">IP address from which the request has initiated</li><li style="text-align: justify;">HTTP method used (GET/POST)</li><li style="text-align: justify;">Uniform Resource Identifier (URI)</li><li style="text-align: justify;">Query sent via HTTP</li><li style="text-align: justify;">HTTP headers</li><li style="text-align: justify;">HTTP request body</li><li style="text-align: justify;">Event logs (non-volatile data)</li><li style="text-align: justify;">File listings and timestamps (non-volatile data)</li></ul><p style="text-align:justify;height:normal;">Most web applications restrict access to HTTP information. Without this, the information recorded in the logs would appear quite similar, which might make it impossible for investigators to differentiate valid HTTP requests from malicious ones.</p></div>
</div></div>